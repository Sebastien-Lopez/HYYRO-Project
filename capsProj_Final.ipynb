{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sebas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sebas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, GRU, Embedding, Dropout, MaxPooling1D, Conv1D, AveragePooling1D, Flatten,Bidirectional,BatchNormalization, LSTM, SpatialDropout1D\n",
    "from keras.initializers import TruncatedNormal\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning words\n",
    "def removeStopWords_Lemmatize(s):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    tempList = [token.lower() for token in tokens if token.lower() not in stopset and  len(token)>2]\n",
    "    tempList = [lemmatizer.lemmatize(w) for w in tempList]\n",
    "    return \" \".join(x for x in tempList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_lengthening(text):\n",
    "\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ad Link</th>\n",
       "      <th>Category</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Maple dining table with chairs (matching Side ...</td>\n",
       "      <td>Beautiful Solid Queensland fiddleback maple di...</td>\n",
       "      <td>$800 negotiable</td>\n",
       "      <td>Bayside Area, Brighton</td>\n",
       "      <td>/s-ad/brighton/antiques/maple-dining-table-wit...</td>\n",
       "      <td>antiques-art-collectables</td>\n",
       "      <td>-37.9081962</td>\n",
       "      <td>144.9957991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Art Deco painting (from original).</td>\n",
       "      <td>Art Deco painting on board with frame from ori...</td>\n",
       "      <td>$600</td>\n",
       "      <td>Glen Eira Area, Bentleigh</td>\n",
       "      <td>/s-ad/bentleigh/art/art-deco-painting-from-ori...</td>\n",
       "      <td>antiques-art-collectables</td>\n",
       "      <td>-37.9185101</td>\n",
       "      <td>145.0409246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Title  \\\n",
       "0           0  Maple dining table with chairs (matching Side ...   \n",
       "1           1                 Art Deco painting (from original).   \n",
       "\n",
       "                                         Description             Price  \\\n",
       "0  Beautiful Solid Queensland fiddleback maple di...   $800 negotiable   \n",
       "1  Art Deco painting on board with frame from ori...             $600    \n",
       "\n",
       "                     Location  \\\n",
       "0      Bayside Area, Brighton   \n",
       "1   Glen Eira Area, Bentleigh   \n",
       "\n",
       "                                             Ad Link  \\\n",
       "0  /s-ad/brighton/antiques/maple-dining-table-wit...   \n",
       "1  /s-ad/bentleigh/art/art-deco-painting-from-ori...   \n",
       "\n",
       "                    Category     Latitude    Longitude  \n",
       "0  antiques-art-collectables  -37.9081962  144.9957991  \n",
       "1  antiques-art-collectables  -37.9185101  145.0409246  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"gumtree Scrapping data.csv\")\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the null values with some self-explanotory text\n",
    "data[\"Description\"]=data[\"Description\"].fillna(\"Undescribed\")\n",
    "data[\"Price\"]=data[\"Price\"].fillna(\"Unpriced\")\n",
    "\n",
    "# filter out for non characters\n",
    "data[\"Title\"]=data[\"Title\"].apply(lambda x: re.sub(r'\\W+', ' ', x))\n",
    "data[\"Description\"]=data[\"Description\"].apply(lambda x: re.sub(r'\\W+', ' ', x))\n",
    "data[\"Price\"]=data[\"Price\"].apply(lambda x: re.sub(r'\\W+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out unrelated words\n",
    "banned = ['and', 'to', 'the', 'a', 'for','with', 'in','is', 'of', 'or', 'on', 'you', 'are', 'from', 'up', 'available'\n",
    "               ,'The', 'Have', 'all', 'at', 'as', 'condition' , 's', 'I', 'your', 'can',  'our', 'new',\n",
    "                'it', 'be', 'We','x', 'This','has','will','only','this', 'New', 'pick','my','an''very', 'if',\n",
    "               'Please', 'also','more','no', 'but', 'Size', 'Melbourne', 'been', 'A', 'store', 'Brand', 'sale', 'any'\n",
    "               ,'great','It', 'well', 'price','All', 'us', 'me', 'so', 'just', 'Australia', 't', 'Hey', \"i'm\", 'hey','hi'\n",
    "         , 'Hi',\"Iâ€™m\", 'negotiable' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: ' '.join([item for item in x.split() if item not in banned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate the Title, Description and Price column\n",
    "data[\"X\"]= data[\"Title\"].apply(str)+\" \"+data[\"Description\"].apply(str)+\" \"+data[\"Price\"].apply(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ' '.join(data[\"X\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 3152), ('to', 2012), ('for', 1794), ('the', 1724), ('a', 1691), ('with', 1688), ('in', 1599), ('is', 1297), ('of', 1192), ('2', 769), ('or', 705), ('on', 702), ('you', 647), ('are', 617), ('from', 611), ('1', 594), ('up', 585), ('5', 563), ('not', 528), ('available', 518), ('4', 495), ('The', 489), ('3', 483), ('s', 483), ('all', 452), ('condition', 447), ('as', 440), ('at', 439), ('have', 436), ('I', 419), ('your', 405), ('negotiable', 395), ('new', 391), ('can', 387), ('6', 380), ('our', 358), ('x', 357), ('it', 338), ('be', 330), ('We', 330), ('listed', 310), ('10', 309), ('New', 291), ('8', 289), ('This', 264), ('has', 258), ('only', 250), ('will', 249), ('by', 237), ('this', 235), ('9', 229), ('Brand', 223), ('Pick', 222), ('7', 222), ('12', 219), ('good', 217), ('we', 215), ('one', 214), ('Size', 210), ('that', 207), ('pick', 199), ('Melbourne', 196), ('sale', 195), ('50', 195), ('Box', 194), ('15', 193), ('an', 192), ('very', 190), ('my', 189), ('20', 188), ('no', 186), ('more', 184), ('A', 183), ('used', 183), ('if', 181), ('White', 181), ('also', 180), ('Please', 180), ('but', 176), ('size', 175), ('room', 174), ('been', 168), ('rent', 164), ('Excellent', 157), ('each', 156), ('store', 156), ('30', 154), ('house', 154), ('All', 152), ('great', 151), ('out', 150), ('any', 149), ('quality', 147), ('It', 147), ('large', 145), ('100', 144), ('price', 143), ('well', 142), ('Australia', 138), ('warranty', 138)]\n"
     ]
    }
   ],
   "source": [
    "# word count to check which words are irrelevent for model\n",
    "words = re.findall('\\w+',out)\n",
    "print(Counter(words).most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove banned words words\n",
    "data[\"X\"] = data[\"X\"].astype(str).apply(f)\n",
    "\n",
    "#Remove Stop words\n",
    "\n",
    "data[\"X\"]=data[\"X\"].apply(lambda x: removeStopWords_Lemmatize(x))\n",
    "\n",
    "#Fix the extra letters like Yesssss\n",
    "data[\"X\"]= data[\"X\"].apply(lambda x: reduce_lengthening(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data[\"X\"]\n",
    "targets= data[\"Category\"].values\n",
    "#targets = targets.reshape((len(targets), 1))\n",
    "\n",
    "tokenizer = Tokenizer( filters='!\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
    "\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize words for model\n",
    "x_train, x_test, y_train, y_test= tts(x,targets,test_size=0.2, random_state=4)\n",
    "\n",
    "x_tr=tokenizer.texts_to_sequences(x_train)\n",
    "x_ts=tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_tr=pad_sequences(x_tr, maxlen=250)\n",
    "x_ts=pad_sequences(x_ts, maxlen=250)\n",
    "\n",
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up transformations\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    y_test_enc = np_utils.to_categorical(y_test_enc)\n",
    "    y_train_enc = np_utils.to_categorical(y_train_enc)\n",
    "    return y_train_enc, y_test_enc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_test=prepare_targets(y_train, y_test)\n",
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 250, 100)          1169100   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 241, 200)          200200    \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, 31, 200)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 7, 200)            0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 256)               351744    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                3084      \n",
      "=================================================================\n",
      "Total params: 1,724,128\n",
      "Trainable params: 1,724,128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "61/61 [==============================] - 25s 61ms/step - loss: 2.4347 - accuracy: 0.1122\n",
      "Epoch 2/5\n",
      "61/61 [==============================] - 4s 59ms/step - loss: 1.4345 - accuracy: 0.5642\n",
      "Epoch 3/5\n",
      "61/61 [==============================] - 4s 59ms/step - loss: 0.3673 - accuracy: 0.8939\n",
      "Epoch 4/5\n",
      "61/61 [==============================] - 4s 60ms/step - loss: 0.0751 - accuracy: 0.9879\n",
      "Epoch 5/5\n",
      "61/61 [==============================] - 4s 59ms/step - loss: 0.0307 - accuracy: 0.9930\n",
      "validation_accuracy is:  0.8132780194282532\n"
     ]
    }
   ],
   "source": [
    "#Neural NetWork Layers\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=vocab_size, output_dim= embedding_dim, input_length=250, embeddings_initializer='TruncatedNormal'))\n",
    "model2.add(Conv1D(200, kernel_size=10,strides=1,activation='relu' ))\n",
    "model2.add(AveragePooling1D(pool_size=8, strides=None, padding='same'))\n",
    "model2.add(MaxPooling1D(pool_size=5, strides=None, padding='same'))\n",
    "model2.add(GRU(256, recurrent_dropout=0.3))\n",
    "#model2.add(Dropout(0.2))\n",
    "model2.add(Dense(12, activation='softmax'))\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model2.summary()\n",
    "\n",
    "\n",
    "\n",
    "model2.fit(x_tr, y_train,epochs=5,verbose=1,batch_size=32)\n",
    "\n",
    "_, val_acc2 = model2.evaluate(x_ts, y_test, verbose=0)\n",
    "print('validation_accuracy is: ', val_acc2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages for Naives bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['antiques-art-collectables',\n",
       " 'baby-children',\n",
       " 'boats-jet-skis',\n",
       " 'books-music-games',\n",
       " 'automotive',\n",
       " 'clothing-jewellery',\n",
       " 'electronics-computer',\n",
       " 'home-garden',\n",
       " 'pets',\n",
       " 'real-estate',\n",
       " 'services-for-hire',\n",
       " 'sport-fitness']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of categories\n",
    "singletargets = list(dict.fromkeys(targets))\n",
    "type(singletargets)\n",
    "singletargets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split train and test\n",
    "x_train, x_test, y_train, y_test= tts(x,targets,test_size=0.2, random_state=4)\n",
    "\n",
    "x_tr=tokenizer.texts_to_sequences(x_train)\n",
    "x_ts=tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_tr=pad_sequences(x_tr, maxlen=250)\n",
    "x_ts=pad_sequences(x_ts, maxlen=250)\n",
    "\n",
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the DTM first\n",
    "cv=CountVectorizer(stop_words='english')\n",
    "train_dtm=cv.fit_transform(x_train)\n",
    "test_dtm=cv.transform(x_test)\n",
    "\n",
    "#Fit the model\n",
    "nb=MultinomialNB()\n",
    "nb.fit(train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes :\n",
      "85.68464730290457\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "predicted=nb.predict(test_dtm)\n",
    "score=nb.score(test_dtm,y_test)\n",
    "print('Accuracy of Naive Bayes :')\n",
    "print(score*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "antiques-art-collectables       0.82      0.75      0.78        36\n",
      "               automotive       0.90      0.98      0.94        46\n",
      "            baby-children       0.80      0.81      0.80        43\n",
      "           boats-jet-skis       0.90      0.97      0.94        38\n",
      "        books-music-games       0.93      0.70      0.80        37\n",
      "       clothing-jewellery       0.88      0.86      0.87        35\n",
      "     electronics-computer       0.90      0.94      0.92        49\n",
      "              home-garden       0.76      0.78      0.77        32\n",
      "                     pets       0.82      0.91      0.86        34\n",
      "              real-estate       0.90      0.94      0.92        47\n",
      "        services-for-hire       0.89      0.80      0.85        41\n",
      "            sport-fitness       0.77      0.77      0.77        44\n",
      "\n",
      "                 accuracy                           0.86       482\n",
      "                macro avg       0.86      0.85      0.85       482\n",
      "             weighted avg       0.86      0.86      0.86       482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report of categories with accuracy \n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_true= y_test,y_pred = predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving Model as a pickle object\n",
    "import pickle\n",
    "f = open('my_classifier.pickle', 'wb')\n",
    "pickle.dump(nb, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the model to use in prediction program\n",
    "f = open('my_classifier.pickle', 'rb')\n",
    "nb = pickle.load(f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
