{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "import keras\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sebas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sebas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sebas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, GRU, Embedding, Dropout, MaxPooling1D, Conv1D, AveragePooling1D, Flatten,Bidirectional,BatchNormalization, LSTM, SpatialDropout1D\n",
    "from keras.initializers import TruncatedNormal\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords_Lemmatize(s):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    tempList = [token.lower() for token in tokens if token.lower() not in stopset and  len(token)>2]\n",
    "    tempList = [lemmatizer.lemmatize(w) for w in tempList]\n",
    "    return \" \".join(x for x in tempList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ad Link</th>\n",
       "      <th>Category</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Maple dining table with chairs (matching Side ...</td>\n",
       "      <td>Beautiful Solid Queensland fiddleback maple di...</td>\n",
       "      <td>$800 negotiable</td>\n",
       "      <td>Bayside Area, Brighton</td>\n",
       "      <td>/s-ad/brighton/antiques/maple-dining-table-wit...</td>\n",
       "      <td>antiques-art-collectables</td>\n",
       "      <td>-37.9081962</td>\n",
       "      <td>144.9957991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Art Deco painting (from original).</td>\n",
       "      <td>Art Deco painting on board with frame from ori...</td>\n",
       "      <td>$600</td>\n",
       "      <td>Glen Eira Area, Bentleigh</td>\n",
       "      <td>/s-ad/bentleigh/art/art-deco-painting-from-ori...</td>\n",
       "      <td>antiques-art-collectables</td>\n",
       "      <td>-37.9185101</td>\n",
       "      <td>145.0409246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Title  \\\n",
       "0           0  Maple dining table with chairs (matching Side ...   \n",
       "1           1                 Art Deco painting (from original).   \n",
       "\n",
       "                                         Description             Price  \\\n",
       "0  Beautiful Solid Queensland fiddleback maple di...   $800 negotiable   \n",
       "1  Art Deco painting on board with frame from ori...             $600    \n",
       "\n",
       "                     Location  \\\n",
       "0      Bayside Area, Brighton   \n",
       "1   Glen Eira Area, Bentleigh   \n",
       "\n",
       "                                             Ad Link  \\\n",
       "0  /s-ad/brighton/antiques/maple-dining-table-wit...   \n",
       "1  /s-ad/bentleigh/art/art-deco-painting-from-ori...   \n",
       "\n",
       "                    Category     Latitude    Longitude  \n",
       "0  antiques-art-collectables  -37.9081962  144.9957991  \n",
       "1  antiques-art-collectables  -37.9185101  145.0409246  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"gumtree Scrapping data.csv\")\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the null values with some self-explanotory text\n",
    "data[\"Description\"]=data[\"Description\"].fillna(\"Undescribed\")\n",
    "data[\"Price\"]=data[\"Price\"].fillna(\"Unpriced\")\n",
    "\n",
    "# filter out for non characters\n",
    "data[\"Title\"]=data[\"Title\"].apply(lambda x: re.sub(r'\\W+', ' ', x))\n",
    "data[\"Description\"]=data[\"Description\"].apply(lambda x: re.sub(r'\\W+', ' ', x))\n",
    "data[\"Price\"]=data[\"Price\"].apply(lambda x: re.sub(r'\\W+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "banned = ['and', 'to', 'the', 'a', 'for','with', 'in','is', 'of', 'or', 'on', 'you', 'are', 'from', 'up', 'available'\n",
    "               ,'The', 'Have', 'all', 'at', 'as', 'condition' , 's', 'I', 'your', 'can',  'our', 'new',\n",
    "                'it', 'be', 'We','x', 'This','has','will','only','this', 'New', 'pick','my','an''very', 'if',\n",
    "               'Please', 'also','more','no', 'but', 'Size', 'Melbourne', 'been', 'A', 'store', 'Brand', 'sale', 'any'\n",
    "               ,'great','It', 'well', 'price','All', 'us', 'me', 'so', 'just', 'Australia', 't', 'Hey', \"i'm\", 'hey','hi'\n",
    "         , 'Hi',\"Iâ€™m\", 'negotiable' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: ' '.join([item for item in x.split() if item not in banned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate the Title, Description and Price column\n",
    "data[\"X\"]= data[\"Title\"].apply(str)+\" \"+data[\"Description\"].apply(str)+\" \"+data[\"Price\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ' '.join(data[\"X\"].astype(str))\n",
    "words = re.findall('\\w+',out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove banned words words\n",
    "data[\"X\"] = data[\"X\"].astype(str).apply(f)\n",
    "\n",
    "#Remove Stop words\n",
    "data[\"X\"]=data[\"X\"].apply(lambda x: removeStopWords_Lemmatize(x))\n",
    "\n",
    "#Fix the extra letters like Yesssss\n",
    "data[\"X\"]= data[\"X\"].apply(lambda x: reduce_lengthening(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data[\"X\"]\n",
    "targets= data[\"Category\"].values\n",
    "#targets = targets.reshape((len(targets), 1))\n",
    "\n",
    "tokenizer = Tokenizer( filters='!\"#$%&()*+,-./:;<=>@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
    "\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test= tts(x,targets,test_size=0.2, random_state=4)\n",
    "\n",
    "x_tr=tokenizer.texts_to_sequences(x_train)\n",
    "x_ts=tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_tr=pad_sequences(x_tr, maxlen=250)\n",
    "x_ts=pad_sequences(x_ts, maxlen=250)\n",
    "\n",
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Create the DTM first\n",
    "cv=CountVectorizer(stop_words='english')\n",
    "train_dtm=cv.fit_transform(x_train)\n",
    "test_dtm=cv.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    y_test_enc = np_utils.to_categorical(y_test_enc)\n",
    "    y_train_enc = np_utils.to_categorical(y_train_enc)\n",
    "    return y_train_enc, y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_test=prepare_targets(y_train, y_test)\n",
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5912863070539419"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from skmultilearn.adapt import MLkNN\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_classifier.fit(train_dtm, y_train)\n",
    "knn_classifier.score(test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5912863070539419\n"
     ]
    }
   ],
   "source": [
    "#Predict testing set\n",
    "y_pred = knn_classifier.predict(test_dtm)\n",
    "#Check performance using accuracy\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "#Check performance using roc\n",
    "# roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06811894882434301"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "metrics.hamming_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5892116182572614"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=700)\n",
    "rf_classifier.fit(train_dtm, y_train)\n",
    "rf_classifier.score(test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6161825726141079"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "eT_classifier = ExtraTreesClassifier(n_estimators=500, max_features = \"sqrt\")\n",
    "eT_classifier.fit(train_dtm, y_train)\n",
    "eT_classifier.score(test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
